# SwissCloudDay2023Philikai
Repository for references, further reading material and code snippets. 



## Parameter Efficient Fine Tuning
Some more reading on LoRa Adapters:
- [Dive into LoRa Adapters](https://towardsdatascience.com/dive-into-lora-adapters-38f4da488ede)
- [Conceptual Guide of HuggingFace for LoRa](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [Original Paper on LoRa - LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)

Jumpstart on SageMaker:
- [Llama 2 foundation models from Meta are now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)
- [Fine-tune Llama 2 for text generation on Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/fine-tune-llama-2-for-text-generation-on-amazon-sagemaker-jumpstart/)

Inferentia on SageMaker:
- [](https://github.com/aws-neuron/aws-neuron-samples/blob/master/torch-neuronx/README.md#inference)
